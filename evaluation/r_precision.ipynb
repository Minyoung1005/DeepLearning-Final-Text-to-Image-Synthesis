{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M2177.003100 Deep Learning <br> Final Project: Text-to-image synthesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submitting your work:\n",
    "<font color=red>**DO NOT clear the R-precision score **</font> so that TAs can grade the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
<<<<<<< HEAD
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/minyoung/anaconda3/envs/deep-learning-21/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:115.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
=======
     "ename": "TabError",
     "evalue": "inconsistent use of tabs and spaces in indentation (data_utils.py, line 59)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/home/minyoung/anaconda3/envs/deep-learning-21/lib/python3.8/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3441\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_16514/1669712416.py\"\u001b[0;36m, line \u001b[0;32m8\u001b[0;36m, in \u001b[0;35m<module>\u001b[0;36m\u001b[0m\n\u001b[0;31m    from utils.data_utils import CUBDataset\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"/home/minyoung/Lectures/DL/21Final/evaluation/../utils/data_utils.py\"\u001b[0;36m, line \u001b[0;32m59\u001b[0m\n\u001b[0;31m    self.imsize = []\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mTabError\u001b[0m\u001b[0;31m:\u001b[0m inconsistent use of tabs and spaces in indentation\n"
>>>>>>> 61c74f7fe1f4996cb4c94417e064ec0c900e6e6e
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys, os, pickle\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "\n",
    "sys.path.append('..')\n",
    "from evaluation.model import CNN_ENCODER, RNN_ENCODER\n",
    "from utils.data_utils import CUBDataset\n",
    "\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "device = torch.device('cpu' if not torch.cuda.is_available() else 'cuda')\n",
    "\n",
    "from miscc.config import cfg, cfg_from_file\n",
<<<<<<< HEAD
    "cfg_from_file('../cfg/eval_birds.yml') #train"
=======
    "cfg_from_file('../cfg/train_birds.yml')"
>>>>>>> 61c74f7fe1f4996cb4c94417e064ec0c900e6e6e
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
=======
   "execution_count": null,
>>>>>>> 61c74f7fe1f4996cb4c94417e064ec0c900e6e6e
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(x1, x2, dim=1, eps=1e-8):\n",
    "    \"\"\"\n",
    "    Returns cosine similarity between x1 and x2, computed along dim\n",
    "    \"\"\"\n",
    "    w12 = torch.sum(x1 * x2, dim)\n",
    "    w1 = torch.norm(x1, 2, dim)\n",
    "    w2 = torch.norm(x2, 2, dim)\n",
    "    return (w12 / (w1 * w2).clamp(min=eps))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
=======
   "execution_count": null,
>>>>>>> 61c74f7fe1f4996cb4c94417e064ec0c900e6e6e
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(imgs, captions, captions_lens, class_ids, keys, captions_idx):\n",
    "    # sort data by the length in a decreasing order\n",
    "    # the reason of sorting data can be found in https://simonjisu.github.io/nlp/2018/07/05/packedsequence.html \n",
    "    sorted_cap_lens, sorted_cap_indices = torch.sort(captions_lens, 0, True)\n",
    "    real_imgs = []\n",
    "    for i in range(len(imgs)):\n",
    "        imgs[i] = imgs[i][sorted_cap_indices]\n",
    "        real_imgs.append(imgs[i])\n",
    "\n",
    "    sorted_captions = captions[sorted_cap_indices].squeeze()\n",
    "    if captions.size(0) == 1:\n",
    "        captions = captions.unsqueeze(0)\n",
    "    sorted_class_ids = class_ids[sorted_cap_indices].numpy()\n",
    "    sorted_keys = [keys[i] for i in sorted_cap_indices.numpy()]\n",
    "    sorted_captions_idx = captions_idx[sorted_cap_indices].numpy()\n",
    "\n",
    "    return [real_imgs, sorted_captions, sorted_cap_lens, sorted_class_ids, sorted_keys, sorted_captions_idx]"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load image encoder\n",
      "Load text encoder\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> 61c74f7fe1f4996cb4c94417e064ec0c900e6e6e
   "source": [
    "image_encoder = CNN_ENCODER(256)\n",
    "state_dict = torch.load('./sim_models/bird/image_encoder.pth', map_location=lambda storage, loc: storage)\n",
    "image_encoder.load_state_dict(state_dict)\n",
    "for p in image_encoder.parameters():\n",
    "    p.requires_grad = False\n",
    "print('Load image encoder')\n",
    "image_encoder.eval()\n",
    "\n",
    "# load the image encoder model to obtain the latent feature of the real caption\n",
    "text_encoder = RNN_ENCODER(5450, nhidden=256)\n",
    "state_dict = torch.load('./sim_models/bird/text_encoder.pth', map_location=lambda storage, loc: storage)\n",
    "text_encoder.load_state_dict(state_dict)\n",
    "for p in text_encoder.parameters():\n",
    "    p.requires_grad = False\n",
    "print('Load text encoder')\n",
    "text_encoder.eval()\n",
    "\n",
    "image_encoder = image_encoder.to(device)\n",
    "text_encoder = text_encoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.current_dir:\n",
      "/home/minyoung/Lectures/DL/21Final/evaluation\n",
      "\n",
      "self.data_dir:\n",
      "/home/minyoung/Lectures/DL/21Final/data/birds\n",
      "\n",
      "self.image_dir:\n",
      "/home/minyoung/Lectures/DL/21Final/data/birds/CUB-200-2011/images\n",
      "\n",
      "filepath /home/minyoung/Lectures/DL/21Final/data/birds/captions.pickle\n",
      "Load from:  /home/minyoung/Lectures/DL/21Final/data/birds/captions.pickle\n",
      "\ttest data directory:\n",
      "/home/minyoung/Lectures/DL/21Final/data/birds/test\n",
      "\n",
      "\t# of test filenames:(2933,)\n",
      "\n",
      "\texample of filename of test image:001.Black_footed_Albatross/Black_Footed_Albatross_0046_18\n",
      "\n",
      "\texample of caption and its ids:\n",
      "['this', 'is', 'a', 'small', 'bird', 'that', 'has', 'a', 'brilliant', 'blue', 'color', 'on', 'it', 's', 'body', 'a', 'slightly', 'darker', 'blue', 'on', 'it', 's', 'head', 'a', 'teal', 'color', 'on', 'it', 's', 'wings', 'and', 'a', 'light', 'colored', 'beak']\n",
      "[18, 19, 1, 250, 2, 33, 13, 1, 853, 50, 37, 86, 53, 54, 15, 1, 178, 31, 50, 86, 53, 54, 25, 1, 1054, 37, 86, 53, 54, 17, 8, 1, 67, 89, 10]\n",
      "\n",
      "\t# of test captions:(29330,)\n",
      "\n",
      "\t# of test caption ids:(29330,)\n",
      "\n",
      "self.current_dir:\n",
      "/home/minyoung/Lectures/DL/21Final/evaluation\n",
      "\n",
      "self.data_dir:\n",
      "/home/minyoung/Lectures/DL/21Final/data/birds\n",
      "\n",
      "self.image_dir:\n",
      "/home/minyoung/Lectures/DL/21Final/data/birds/CUB-200-2011/images\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31951/2847039090.py:11: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  print(f'\\t# of test captions:{np.asarray(test_dataset.captions).shape}\\n')\n",
      "/tmp/ipykernel_31951/2847039090.py:12: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  print(f'\\t# of test caption ids:{np.asarray(test_dataset.captions_ids).shape}\\n')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filepath /home/minyoung/Lectures/DL/21Final/data/birds/captions.pickle\n",
      "Load from:  /home/minyoung/Lectures/DL/21Final/data/birds/captions.pickle\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> 61c74f7fe1f4996cb4c94417e064ec0c900e6e6e
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128))\n",
    "])\n",
    "\n",
    "test_dataset = CUBDataset(cfg.DATA_DIR, transform=transform, split='test', eval_mode=True)\n",
    "\n",
    "print(f'\\ttest data directory:\\n{test_dataset.split_dir}\\n')\n",
    "print(f'\\t# of test filenames:{test_dataset.filenames.shape}\\n')\n",
    "print(f'\\texample of filename of test image:{test_dataset.filenames[0]}\\n')\n",
    "print(f'\\texample of caption and its ids:\\n{test_dataset.captions[0]}\\n{test_dataset.captions_ids[0]}\\n')\n",
    "print(f'\\t# of test captions:{np.asarray(test_dataset.captions).shape}\\n')\n",
    "print(f'\\t# of test caption ids:{np.asarray(test_dataset.captions_ids).shape}\\n')\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=cfg.BATCH_SIZE,\n",
    "                                              drop_last=False, shuffle=False, num_workers=int(cfg.WORKERS))\n",
    "\n",
    "test_dataset_for_wrong = CUBDataset(cfg.DATA_DIR, transform=transform, split='test', eval_mode=True, for_wrong=True)\n",
    "current_dir = os.getcwd()\n",
    "fname = os.path.join(current_dir.replace('/evaluation', ''), cfg.DATA_DIR, 'test', 'test_caption_info.pickle')\n",
    "with open(fname, 'rb') as f:\n",
    "    test_caption_info = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31951/3691870953.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  n_data = np.asarray(test_dataset.captions_ids).shape[0]\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> 61c74f7fe1f4996cb4c94417e064ec0c900e6e6e
   "source": [
    "n_data = np.asarray(test_dataset.captions_ids).shape[0]\n",
    "true_cnn_features = np.zeros((n_data, cfg.TEXT.EMBEDDING_DIM), dtype=float)\n",
    "true_rnn_features = np.zeros((n_data, cfg.TEXT.EMBEDDING_DIM), dtype=float)\n",
    "wrong_rnn_features = np.zeros((n_data, cfg.WRONG_CAPTION, cfg.TEXT.EMBEDDING_DIM), dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
<<<<<<< HEAD
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/home/minyoung/Lectures/DL/21Final/utils/data_utils.py\u001b[0m(289)\u001b[0;36mget_caption\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    288 \u001b[0;31m        \u001b[0;32mimport\u001b[0m \u001b[0mipdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 289 \u001b[0;31m        \u001b[0msent_caption\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptions_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msent_ix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'int64'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    290 \u001b[0;31m        \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msent_caption\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m> \u001b[0;32m/home/minyoung/Lectures/DL/21Final/utils/data_utils.py\u001b[0m(289)\u001b[0;36mget_caption\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    288 \u001b[0;31m        \u001b[0;32mimport\u001b[0m \u001b[0mipdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 289 \u001b[0;31m        \u001b[0msent_caption\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptions_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msent_ix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'int64'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    290 \u001b[0;31m        \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msent_caption\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "\n",
      "> \u001b[0;32m/home/minyoung/Lectures/DL/21Final/utils/data_utils.py\u001b[0m(289)\u001b[0;36mget_caption\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    288 \u001b[0;31m        \u001b[0;32mimport\u001b[0m \u001b[0mipdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 289 \u001b[0;31m        \u001b[0msent_caption\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptions_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msent_ix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'int64'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    290 \u001b[0;31m        \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msent_caption\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m> \u001b[0;32m/home/minyoung/Lectures/DL/21Final/utils/data_utils.py\u001b[0m(289)\u001b[0;36mget_caption\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    288 \u001b[0;31m        \u001b[0;32mimport\u001b[0m \u001b[0mipdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 289 \u001b[0;31m        \u001b[0msent_caption\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptions_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msent_ix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'int64'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    290 \u001b[0;31m        \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msent_caption\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "\n"
     ]
    }
   ],
=======
   "outputs": [],
>>>>>>> 61c74f7fe1f4996cb4c94417e064ec0c900e6e6e
   "source": [
    "total_cs = []\n",
    "cnn_features = []\n",
    "for batch_idx, data in enumerate(test_dataloader):\n",
    "    gen_imgs = data['gen_img']\n",
    "    captions = data['caps']\n",
    "    captions_lens = data['cap_len']\n",
    "    class_ids = data['cls_id']\n",
    "    keys = data['key']\n",
    "    captions_idx = data['cap_ix']\n",
    "    \n",
    "    sorted_gen_imgs, sorted_captions, sorted_cap_lens, sorted_class_ids, sorted_keys, sorted_captions_idx = prepare_data(gen_imgs, captions, captions_lens, class_ids, keys, captions_idx)\n",
    "    \n",
    "    if cfg.CUDA:\n",
    "        sorted_captions = sorted_captions.to(device)\n",
    "        sorted_cap_lens = sorted_cap_lens.to(device)\n",
    "    \n",
    "    hidden = text_encoder.init_hidden(sorted_captions.size(0))\n",
    "    _, sent_emb = text_encoder(sorted_captions, sorted_cap_lens, hidden)\n",
    "\n",
    "    _, gen_sent_code = image_encoder(sorted_gen_imgs[-1].to(device))\n",
    "\n",
    "    true_sim = cosine_similarity(gen_sent_code, sent_emb)\n",
    "    \n",
    "    true_cnn_features[captions_idx] = gen_sent_code.detach().cpu().numpy()\n",
    "    true_rnn_features[captions_idx] = sent_emb.detach().cpu().numpy()\n",
    "    \n",
    "    false_sim_list = []\n",
    "    for i in range(captions.size(0)):\n",
    "        assert sorted_class_ids[i] == test_caption_info[sorted_captions_idx[i]][0]\n",
    "        cap_cls_id = test_caption_info[sorted_captions_idx[i]][1]\n",
    "        mis_match_captions, sorted_mis_cap_lens = test_dataset_for_wrong.get_mis_captions(sorted_class_ids[i], cap_cls_id)\n",
    "        \n",
    "        if cfg.CUDA:\n",
    "            mis_match_captions = mis_match_captions.to(device)\n",
    "            sorted_mis_cap_lens = sorted_mis_cap_lens.to(device)\n",
    "        \n",
    "        mis_hidden = text_encoder.init_hidden(mis_match_captions.size(0))\n",
    "        _, mis_sent_emb = text_encoder(mis_match_captions, sorted_mis_cap_lens, mis_hidden)\n",
    "\n",
    "        false_sim = cosine_similarity(gen_sent_code[i:i+1], mis_sent_emb)\n",
    "        \n",
    "        wrong_rnn_features[captions_idx] = mis_sent_emb.detach().cpu().numpy()\n",
    "        \n",
    "        false_sim_list.append(false_sim)\n",
    "    \n",
    "    batch_cs = torch.cat([torch.unsqueeze(true_sim, 1), torch.stack(false_sim_list, dim=0)], dim=1)\n",
    "    total_cs.append(batch_cs)\n",
    "\n",
    "total_cs = torch.cat(total_cs, dim=0)\n",
    "r_precision = torch.mean((torch.argmax(total_cs, dim=1) == 0) * 1.0).cpu().detach().numpy()\n",
    "print('# images for evaluation:', len(total_cs))\n",
    "print('R-precision: ' + str(r_precision))\n",
    "\n",
    "np.savez(cfg.R_PRECISION_FILE, total_cs=total_cs.detach().cpu().numpy(),\n",
    "         true_cnn_features=true_cnn_features, true_rnn_features=true_rnn_features, wrong_rnn_features=wrong_rnn_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deep-learning-21] *",
   "language": "python",
   "name": "conda-env-deep-learning-21-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
